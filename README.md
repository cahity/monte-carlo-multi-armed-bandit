## Monte carlo simulation for basic multi-armed bandit approximations with graduate school expectation storyline.

Included methods are:

 - Max reward (for comparison)
 - Explore only
 - Exploit only
 - $\epsilon$-Greedy

Regret is also calculated.
